{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"EX_7_Simple_ANN_Network.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyN0vWxdBX/dKjGBduR/kqvt"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"markdown","metadata":{"id":"xcuthNs-n1AA"},"source":["##Neural Network Implementation in Python\n"]},{"cell_type":"code","metadata":{"id":"ap3YjQlqaUCu"},"source":["import numpy as np\n","feature_set = np.array([[0,1,0],[0,0,1],[1,0,0],[1,1,0],[1,1,1]])\n","labels = np.array([[1,0,0,1,1]])\n","labels = labels.reshape(5,1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2paXPFq2n8ss"},"source":["#####we create our feature set. It contains five records. Similarly, we created a labels set which contains corresponding labels for each record in the feature set. The labels are the answers we're trying to predict with the neural network.\n","\n","The next step is to define hyper parameters for our neural network. Execute the following script to do so:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPLjB7EJkRfX","executionInfo":{"status":"ok","timestamp":1606312128614,"user_tz":-330,"elapsed":1205,"user":{"displayName":"ER_Vishvanath Metkari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN8UEUH_js3bblgQbWf69uo4Tkv7gN5uirnD-d7g=s64","userId":"16293190159080865619"}},"outputId":"1a98cfbb-6948-4085-b133-c87c885c21d9"},"source":["feature_set"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0, 1, 0],\n","       [0, 0, 1],\n","       [1, 0, 0],\n","       [1, 1, 0],\n","       [1, 1, 1]])"]},"metadata":{"tags":[]},"execution_count":30}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B26hqqQXkeY6","executionInfo":{"status":"ok","timestamp":1606312130471,"user_tz":-330,"elapsed":1293,"user":{"displayName":"ER_Vishvanath Metkari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN8UEUH_js3bblgQbWf69uo4Tkv7gN5uirnD-d7g=s64","userId":"16293190159080865619"}},"outputId":"91dd77bb-7c20-4da0-a32d-6bd1b131720d"},"source":["labels"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[1],\n","       [0],\n","       [0],\n","       [1],\n","       [1]])"]},"metadata":{"tags":[]},"execution_count":31}]},{"cell_type":"code","metadata":{"id":"hsXh1gFUknI5"},"source":["np.random.seed(0)\n","weights = np.random.rand(3,1)\n","bias = np.random.rand(1)\n","lr = 0.05"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzXCRxqcoCv9"},"source":["#####In the script above we used the random.seed function so that we can get the same random values whenever the script is executed.\n","\n","In the next step, we initialize our weights with normally distributed random numbers. Since we have three features in the input, we have a vector of three weights. We then initialize the bias value with another random number. Finally, we set the learning rate to 0.05.\n","\n","Next, we need to define our activation function and its derivative (I'll explain in a moment why we need to find the derivative of the activation). Our activation function is the sigmoid function, which we covered earlier."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mIzsRxs2kw9f","executionInfo":{"status":"ok","timestamp":1606312138871,"user_tz":-330,"elapsed":1263,"user":{"displayName":"ER_Vishvanath Metkari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN8UEUH_js3bblgQbWf69uo4Tkv7gN5uirnD-d7g=s64","userId":"16293190159080865619"}},"outputId":"ad24f07a-6cad-4794-f75b-9c44a87b1355"},"source":["weights"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[0.5488135 ],\n","       [0.71518937],\n","       [0.60276338]])"]},"metadata":{"tags":[]},"execution_count":33}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bbz-815-kzAc","executionInfo":{"status":"ok","timestamp":1606312142038,"user_tz":-330,"elapsed":1097,"user":{"displayName":"ER_Vishvanath Metkari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN8UEUH_js3bblgQbWf69uo4Tkv7gN5uirnD-d7g=s64","userId":"16293190159080865619"}},"outputId":"df6655d5-2fa3-4f8d-c001-d9c7c3a8f2db"},"source":["bias"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([0.54488318])"]},"metadata":{"tags":[]},"execution_count":34}]},{"cell_type":"markdown","metadata":{"id":"Uypbo9LAof86"},"source":["#####Activation Function \n","\n"]},{"cell_type":"code","metadata":{"id":"11YQC_Pek463"},"source":["def sigmoid(x):\n","    return 1/(1+np.exp(-x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"199UTWbDodY1"},"source":["#####calculates the derivative of the sigmoid function "]},{"cell_type":"code","metadata":{"id":"MSQ4Rw0Kk9hH"},"source":["def sigmoid_der(x):\n","    return sigmoid(x)*(1-sigmoid(x))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5eQKEhaLlvr2"},"source":["for epoch in range(50000):\n","    inputs = feature_set\n","\n","    # feedforward step1\n","    XW = np.dot(feature_set, weights) + bias\n","\n","    #feedforward step2\n","    z = sigmoid(XW)\n","\n","\n","    # backpropagation step 1\n","    error = z - labels\n","\n","    print(error.sum())\n","\n","    # backpropagation step 2\n","    dcost_dpred = error\n","    dpred_dz = sigmoid_der(z)\n","\n","    z_delta = dcost_dpred * dpred_dz\n","\n","    inputs = feature_set.T\n","    weights -= lr * np.dot(inputs, z_delta)\n","\n","    for num in z_delta:\n","        bias -= lr * num"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BH2x9oRBo3Je"},"source":["######In the first step, we define the number of epochs. An epoch is basically the number of times we want to train the algorithm on our data. We will train the algorithm on our data 50,000 times. I have tested this number and found that the error is pretty much minimized after 50,000 iterations. You can try with a different number. The ultimate goal is to minimize the error.\n","\n","Next we store the values from the feature_set to the input variable. \n","\n","Then find the dot product of the input and the weight vector and add bias to it. This is Step 1 of the feedforward section.\n","\n","Then We pass the dot product through the sigmoid activation function, as explained in Step 2 of the feedforward section. This completes the feed forward part of our algorithm.\n","\n","Now is the time to start backpropagation. The variable z contains the predicted outputs. The first step of the backpropagation is to find the error.\n","\n","We need to differentiate this function with respect to each weight. We will use the chain rule of differentiation for this purpose. Let's suppose \"d_cost\" is the derivate of our cost function with respect to weight \"w\", we can use chain rule to find this derivative.\n","\n","Here \"d_pred\" is simply the sigmoid function and we have differentiated it with respect to input dot product \"z\".\n","\n","Therefore, derivative with respect to any weight is simply the corresponding input. Hence, our final derivative of the cost function with respect to any weight\n","\n","Here we have the z_delta variable, which contains the product of dcost_dpred and dpred_dz. Instead of looping through each record and multiplying the input with corresponding z_delta, we take the transpose of the input feature matrix and multiply it with the z_delta. Finally, we multiply the learning rate variable lr with the derivative to increase the speed of convergence.\n","\n","We then looped through each derivative value and update our bias values"]},{"cell_type":"markdown","metadata":{"id":"5s6g_ibpo5rU"},"source":["You can see that error is extremely small at the end of the training of our neural network. At this point of time our weights and bias will have values that can be used to detect whether a person is diabetic or not, based on his smoking habits, obesity, and exercise habits.\n","\n","You can now try and predict the value of a single instance. Let's suppose we have a record of a patient that comes in who smokes, is not obese, and doesn't exercise. Let's find if he is likely to be diabetic or not. The input feature will look like this: [1,0,0]."]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U6OiCj6emtaY","executionInfo":{"status":"ok","timestamp":1606312177727,"user_tz":-330,"elapsed":1235,"user":{"displayName":"ER_Vishvanath Metkari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN8UEUH_js3bblgQbWf69uo4Tkv7gN5uirnD-d7g=s64","userId":"16293190159080865619"}},"outputId":"99953f1f-54a4-47c0-9be5-12e512c93edd"},"source":["single_point = np.array([1,0,0])\n","result = sigmoid(np.dot(single_point, weights) + bias)\n","print(result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.00281444]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GwL4gGrcnbEY","executionInfo":{"status":"ok","timestamp":1606312180547,"user_tz":-330,"elapsed":1195,"user":{"displayName":"ER_Vishvanath Metkari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN8UEUH_js3bblgQbWf69uo4Tkv7gN5uirnD-d7g=s64","userId":"16293190159080865619"}},"outputId":"c86d2bf6-ef45-4896-8525-74c0ca0d1361"},"source":["single_point = np.array([0,1,0])\n","result = sigmoid(np.dot(single_point, weights) + bias)\n","print(result)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["[0.99938023]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"TnAQo2mhnmk5","executionInfo":{"status":"ok","timestamp":1606312184849,"user_tz":-330,"elapsed":1622,"user":{"displayName":"ER_Vishvanath Metkari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN8UEUH_js3bblgQbWf69uo4Tkv7gN5uirnD-d7g=s64","userId":"16293190159080865619"}},"outputId":"47d2c698-613f-4de2-d38b-3b73097b10d9"},"source":["weights"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([[-0.51534317],\n","       [12.74036917],\n","       [-0.86631273]])"]},"metadata":{"tags":[]},"execution_count":40}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H_laNx9PoMNF","executionInfo":{"status":"ok","timestamp":1606312189193,"user_tz":-330,"elapsed":1016,"user":{"displayName":"ER_Vishvanath Metkari","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiN8UEUH_js3bblgQbWf69uo4Tkv7gN5uirnD-d7g=s64","userId":"16293190159080865619"}},"outputId":"3fde15e8-5ebf-416a-fb93-da833a15fefc"},"source":["bias"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-5.35483198])"]},"metadata":{"tags":[]},"execution_count":41}]},{"cell_type":"code","metadata":{"id":"OkpiCVexqtgJ"},"source":[""],"execution_count":null,"outputs":[]}]}